[executables]
average_psd = ${which:pycbc_average_psd}
bank2hdf = ${which:pycbc_coinc_bank2hdf}
calculate_psd = ${which:pycbc_calculate_psd}
coinc = ${which:pycbc_coinc_findtrigs}
combine_statmap = ${which:pycbc_add_statmap}
exclude_zerolag = ${which:pycbc_exclude_zerolag}
fit_by_template = ${which:pycbc_fit_sngls_by_template}
fit_over_param = ${which:pycbc_fit_sngls_over_multiparam}
foreground_censor = ${which:pycbc_foreground_censor}
hdfinjfind = ${which:pycbc_coinc_hdfinjfind}
hdf_trigger_merge = ${which:pycbc_coinc_mergetrigs}
inj2hdf = ${which:pycbc_convertinjfiletohdf}
inj_cut = ${which:pycbc_inj_cut}
injections = ${which:lalapps_inspinj}
inspiral = ${which:pycbc_inspiral}
merge_psds = ${which:pycbc_merge_psds}
optimal_snr = ${which:pycbc_optimal_snr}
optimal_snr_merge = ${which:pycbc_merge_inj_hdf}
page_foreground = ${which:pycbc_page_foreground}
page_ifar = ${which:pycbc_page_ifar}
page_ifar_catalog = ${which:pycbc_ifar_catalog}
page_injections = ${which:pycbc_page_injtable}
page_segplot = ${which:pycbc_page_segplot}
page_segtable = ${which:pycbc_page_segtable}
page_versioning = ${which:pycbc_page_versioning}
page_vetotable = ${which:pycbc_page_vetotable}
plot_bank = ${which:pycbc_plot_bank_bins}
plot_binnedhist = ${which:pycbc_fit_sngls_split_binned}
plot_coinc_snrchi = ${which:pycbc_page_coinc_snrchi}
plot_foundmissed = ${which:pycbc_page_foundmissed}
plot_gating = ${which:pycbc_plot_gating}
plot_hist = ${which:pycbc_plot_hist}
plot_qscan = ${which:pycbc_plot_qscan}
plot_range = ${which:pycbc_plot_range}
plot_segments = ${which:pycbc_page_segments}
plot_sensitivity = ${which:pycbc_page_sensitivity}
plot_singles = ${which:pycbc_plot_singles_vs_params}
plot_snrchi = ${which:pycbc_page_snrchi}
plot_snrifar = ${which:pycbc_page_snrifar}
plot_spectrum = ${which:pycbc_plot_psd_file}
plot_throughput = ${which:pycbc_plot_throughput}
results_page = ${which:pycbc_make_html_page}
splitbank = ${which:pycbc_hdf5_splitbank}
statmap = /home/rdhurkun/searches/hyperbolic-search/test-search/kanchan_test/modified_scripts/pycbc_coinc_statmap
statmap_inj = ${which:pycbc_coinc_statmap_inj}
strip_injections = ${which:pycbc_strip_injections}
tmpltbank = ${which:pycbc_geom_nonspinbank}
html_snippet = ${which:pycbc_create_html_snippet}
foreground_minifollowup = ${which:pycbc_foreground_minifollowup}
injection_minifollowup = ${which:pycbc_injection_minifollowup}
singles_minifollowup = ${which:pycbc_sngl_minifollowup}
page_injinfo = ${which:pycbc_page_injinfo}
page_coincinfo = ${which:pycbc_page_coincinfo}
page_snglinfo = ${which:pycbc_page_snglinfo}
plot_trigger_timeseries = ${which:pycbc_plot_trigger_timeseries}
single_template_plot = ${which:pycbc_single_template_plot}
single_template = ${which:pycbc_single_template}
plot_singles_timefreq = ${which:pycbc_plot_singles_timefreq}
plot_snrratehist = ${which:pycbc_page_snrratehist}
plot_waveform = ${which:pycbc_plot_waveform}
page_farstat = ${which:pycbc_page_fars_vs_stat}

; #################### Executable Memory Requirements ########################
[pegasus_profile]
; This section contains default profile information for every job
; This is overriden by profile information set for specific job types
; condor|accounting_group = sugwg.astro
pycbc|primary_site = condorpool_symlink
pycbc|submit-directory = ./
; This sets the initial memory footprint request
condor|+InitialRequestMemory = 1980
; Use the initial request unless the job has been evicted for using too much
; memory. In that case, ask for 50% more than the last resident memory use,
; ramping up by the number of job starts.
condor|request_memory = ifthenelse( (LastHoldReasonCode=!=34 && LastHoldReasonCode=!=26), InitialRequestMemory, int(2 * NumJobStarts * MemoryUsage) )
condor|request_disk = 2000001
; If the job has been held for using too much memory, release it and let the
; memory request bump take effect
condor|periodic_release = ((HoldReasonCode =?= 34) || (HoldReasonCode =?= 26))
condor|+MaxRunTimeHours = 5
env|GWDATAFIND_SERVER='datafind.gw-openscience.org'

[pegasus_profile-condorpool_shared]
pycbc|site-scratch = ./
pycbc|unique-scratch = 
dagman|priority = 10000
;condor|request_disk = 5000

; #################### Pegasus Configuration for Executable ##################
[pegasus_profile-inspiral]
condor|+InitialRequestMemory = 5000
condor|periodic_hold = (JobStatus == 2) && ((CurrentTime - EnteredCurrentStatus) > (2 * 86400))
condor|periodic_release = ((HoldReasonCode =?= 34) || (HoldReasonCode =?= 26)) || ((JobStatus == 5) && (HoldReasonCode == 3) && (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > ( 300)))
condor|periodic_remove = (NumJobStarts >= 5)
;condor|request_disk = 1000
dagman|retry = 10000
dagman|priority = 10000

[pegasus_profile-calculate_psd]
condor|+InitialRequestMemory = 9000
;condor|request_disk = 10000
condor|request_cpus = ${calculate_psd|cores}
dagman|priority = 10000
dagman|retry = 10

[pegasus_profile-hdf_trigger_merge]
dagman|priority = 5000
condor|request_memory = 50GB
; condorpool_shared may not be necessary for this job as we want to use the disk space all at once ; this option can be slower as it is limited by the network speed
pycbc|site= condorpool_shared

[pegasus_profile-fit_by_template]
condor|request_memory = 10GB

[pegasus_profile-fit_over_param]
condor|request_memory = 10GB

[pegasus_profile-coinc]
; use different site options to check that they work
condor|request_memory = 30GB
#using 51.2gb of mem raise to 60GB
#condor|request_disk = 30356988
condor|request_cpus = 2
pycbc|site = condorpool_shared

[pegasus_profile-bank2hdf]
dagman|priority = 5000

[pegasus_profile-merge_psds]
dagman|priority = 2000
condor|+MaxRunTimeHours = 15

[pegasus_profile-statmap]
condor|request_memory = 10GB
pycbc|site= condorpool_shared

[pegasus_profile-results_page]
pycbc|site = condorpool_shared
